{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 使用GPT2示例\n",
   "id": "22e2dea90ecabd21"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "# 初始化分词器和模型\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "# 待向量化的文本\n",
    "text = \"LLM with me\"\n",
    "# 分词并转化为索引\n",
    "input = tokenizer(text, return_tensors=\"pt\")\n",
    "print(input)\n"
   ],
   "id": "f20641c8595cd1d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "text = \"LLM with me\"\n",
    "input = tokenizer(text, return_tensors=\"pt\")\n",
    "print(input)"
   ],
   "id": "861c242f7e16faf3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "text = \"LLM\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "print(inputs)\n",
    "\n",
    "# 查看索引对应的token\n",
    "print(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0]))"
   ],
   "id": "7ec1783db84ee37a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 获取GPT2分词器的长度\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "print(f\"分词器的词汇表长度为：{len(tokenizer)}\")"
   ],
   "id": "650888ae68c69a32",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "\n",
    "# 初始化分词器和模型\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "# 待处理的文本\n",
    "text = \"LLM with me\"\n",
    "# 分词并转换为索引\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "# 获取模型的嵌入层\n",
    "embeddings = model.get_input_embeddings()\n",
    "# 将索引转换为嵌入向量\n",
    "input_embeddings = embeddings(input_ids)\n",
    "print(input_embeddings)\n",
    "print(input_embeddings.shape)\n"
   ],
   "id": "245b87e90d14883c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T13:29:21.296362Z",
     "start_time": "2025-04-02T13:29:21.289891Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from   tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "# 创建一个空的BPE分词器，使用空格进行预分词，闯进啊一个分词器训练器\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "# 添加两个占位来解决不认识的词和结束语\n",
    "trainer = BpeTrainer(special_tokens=['[UNK]',\"<EOS>\"])\n",
    "\n",
    "# 准备一些训练数据\n",
    "train_data = [\"LLM with me\",\"I love learning\",\"I want to drink water\"]\n",
    "# 训练分词器\n",
    "tokenizer.train_from_iterator(train_data,trainer)\n",
    "#保存分词器到文件\n",
    "tokenizer.save(\"custom_tokenizer.json\")\n",
    "#测试分词器\n",
    "output = tokenizer.encode(str(train_data))\n",
    "print(output.tokens)"
   ],
   "id": "6066187ee39e49fb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[UNK]', '[UNK]', 'LLM', 'with', 'me', '[UNK]', '[UNK]', '[UNK]', 'I', 'love', 'learning', '[UNK]', '[UNK]', '[UNK]', 'I', 'want', 'to', 'drink', 'water', '[UNK]', '[UNK]']\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T13:36:45.725542Z",
     "start_time": "2025-04-02T13:36:45.495756Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "input_ids = output.ids\n",
    "print(input_ids)\n",
    "# 加载自定义分词器，编码文本并返回pytorch张量\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"custom_tokenizer.json\")\n",
    "inputs = tokenizer(text,return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "print(input_ids)\n"
   ],
   "id": "289ca2db1a967e30",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 40, 44, 30, 0, 0, 0, 2, 42, 45, 0, 0, 0, 2, 38, 33, 41, 39, 0, 0]\n",
      "tensor([[40, 44, 30]])\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T13:44:30.292307Z",
     "start_time": "2025-04-02T13:44:29.574706Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import PreTrainedTokenizerFast, GPT2Config,GPT2Model\n",
    "# 加载自定义分词器\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"custom_tokenizer.json\")\n",
    "#  创建模型配置以及初始化模型\n",
    "config = GPT2Config(vocab_size=tokenizer.vocab_size,n_embd=768,n_layer=12,n_head=12)\n",
    "model = GPT2Model(config=config)\n",
    "# 待处理的文本 & 分词并转换为索引\n",
    "text = \"LLM with me\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "# 获取模型的嵌入层 & 将索引转换为嵌入向量\n",
    "embeddings = model.get_input_embeddings()\n",
    "input_embeddings = embeddings(input_ids)\n",
    "print(input_embeddings)\n",
    "print(input_embeddings.shape)"
   ],
   "id": "1e5c9a5537a7a403",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0059,  0.0014,  0.0045,  ...,  0.0042,  0.0058,  0.0277],\n",
      "         [-0.0076, -0.0321,  0.0156,  ...,  0.0027, -0.0430, -0.0204],\n",
      "         [-0.0022,  0.0157,  0.0055,  ...,  0.0270,  0.0320, -0.0338]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "torch.Size([1, 3, 768])\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T13:49:19.278967Z",
     "start_time": "2025-04-02T13:49:19.255061Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"custom_tokenizer.json\")\n",
    "# 待处理的文本 & 分词并转换为索引\n",
    "text = \"LLM with me\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "#假设我们的自定义分词器有一个很小的词汇表\n",
    "vocab_size = tokenizer.vocab_size  # 从自定义分词器获取词汇表大小\n",
    "n_embd = 10  # 设置嵌入维度为10\n",
    "# 创建一个随机初始化的嵌入矩阵，这里我们使用正太分布随机初始化，与实际模型初始化类似\n",
    "embedding_matrix = torch.randn(vocab_size, n_embd)\n",
    "# 假设input_ids是一个包含索引的张量\n",
    "token_indices = input_ids[0]\n",
    "token_embeddings = embedding_matrix[token_indices]\n",
    "print(token_embeddings)\n",
    "print(token_embeddings.shape)\n"
   ],
   "id": "35442f81ae419ef8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5592, -0.1788,  1.1371,  1.9707, -0.8604,  1.8147,  1.2781, -0.2853,\n",
      "         -1.2177,  1.0596],\n",
      "        [ 1.3030,  0.8386, -0.9091, -0.5357,  1.6616,  0.9948,  0.6228,  0.3903,\n",
      "         -0.7626, -1.4527],\n",
      "        [ 1.7207,  1.8723,  0.7415,  0.2367, -1.9379, -0.2421,  0.8787,  0.8633,\n",
      "          0.2912, -0.4687]])\n",
      "torch.Size([3, 10])\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T13:51:00.751996Z",
     "start_time": "2025-04-02T13:51:00.747968Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(embedding_matrix[13])\n",
    "print(embedding_matrix[14])\n",
    "print(embedding_matrix[11])"
   ],
   "id": "5c9d17f4c5827901",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3.0015, -1.3700, -0.7345, -1.2543,  0.3699,  0.4769,  0.2865, -1.6468,\n",
      "         1.1635,  0.4964])\n",
      "tensor([ 1.2094,  1.0582, -1.9204, -0.0973, -0.5215,  1.7699, -0.5906,  1.5935,\n",
      "         1.0168,  0.5269])\n",
      "tensor([ 0.1870, -1.9807, -0.0842,  1.0848,  0.9552, -0.2962,  1.0483,  0.3700,\n",
      "        -0.8440,  0.3670])\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T13:56:55.449745Z",
     "start_time": "2025-04-02T13:56:55.262690Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "vocab_size = tokenizer.vocab_size; n_embd = 10\n",
    "embedding_matrix = torch.empty(vocab_size, n_embd)\n",
    "nn.init.xavier_uniform_(embedding_matrix)\n",
    "# 定义一个简化版的GPT模型\n",
    "class SimpleGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd):\n",
    "        super(SimpleGPT, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, n_embd)\n",
    "        self.ffn = nn.Linear(n_embd, n_embd)\n",
    "        self.logits = nn.Linear(n_embd, vocab_size)\n",
    "        nn.init.xavier_uniform_(self.embeddings.weight)  # 使用Xavier初始化嵌入层\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        x = self.embeddings(input_ids)  # 嵌入层\n",
    "        x = self.ffn(x)  # 前馈网络\n",
    "        logits = self.logits(x)  # 输出层\n",
    "        return logits\n",
    "\n",
    "# 创建模型实例 & 定义损失函数和优化器\n",
    "model = SimpleGPT(vocab_size, n_embd)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 假设我们有一些训练数据\n",
    "input_ids = torch.tensor([[1, 2, 3, 4], [2, 3, 4, 5]])  # 示例输入\n",
    "labels = torch.tensor([[2, 3, 4, 5], [3, 4, 5, 6]])  # 示例目标\n",
    "\n",
    "# 训练循环\n",
    "for epoch in range(100):  # 假设训练100个epoch\n",
    "    logits = model(input_ids)  # 前向传播\n",
    "    loss = loss_fn(logits.view(-1, vocab_size), labels.view(-1))  # 计算损失\n",
    "    # 反向传播\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # 梯度下降，根据loss值去更新模型的参数，里面则包括embeddings\n",
    "    optimizer.step()\n",
    "    # 打印损失\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch {epoch + 1}, Loss: {loss.item()}')\n"
   ],
   "id": "ef7bce3898dca51f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 3.9529201984405518\n",
      "Epoch 20, Loss: 3.8673152923583984\n",
      "Epoch 30, Loss: 3.779238700866699\n",
      "Epoch 40, Loss: 3.686739206314087\n",
      "Epoch 50, Loss: 3.58780837059021\n",
      "Epoch 60, Loss: 3.480212926864624\n",
      "Epoch 70, Loss: 3.3615834712982178\n",
      "Epoch 80, Loss: 3.2297677993774414\n",
      "Epoch 90, Loss: 3.083065986633301\n",
      "Epoch 100, Loss: 2.9204678535461426\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T13:58:00.642206Z",
     "start_time": "2025-04-02T13:58:00.638445Z"
    }
   },
   "cell_type": "code",
   "source": [
    "token_indices = input_ids[0]\n",
    "token_embeddings = model.embeddings(token_indices)\n",
    "print(token_embeddings)\n",
    "print(token_embeddings.shape)"
   ],
   "id": "7ed60c60b235bdab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0508,  0.3242, -0.3024,  0.0752, -0.4042,  0.4311,  0.1327,  0.1756,\n",
      "         -0.1218,  0.2336],\n",
      "        [-0.3098,  0.1813, -0.0902,  0.2466, -0.1132,  0.3045, -0.0063, -0.1865,\n",
      "         -0.0918,  0.2803],\n",
      "        [-0.1231,  0.2970,  0.2392, -0.1095,  0.4116,  0.1114, -0.1603,  0.2977,\n",
      "         -0.0465,  0.4548],\n",
      "        [-0.4237, -0.2317, -0.4082,  0.0113, -0.1408,  0.4488, -0.1965, -0.1343,\n",
      "         -0.0252,  0.2996]], grad_fn=<EmbeddingBackward0>)\n",
      "torch.Size([4, 10])\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T14:00:37.192779Z",
     "start_time": "2025-04-02T14:00:37.183692Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn.functional as F\n",
    "# 假设model是训练好的模型实例 & 假设tokenizer是加载好的分词器\n",
    "model.eval() # 将模型设置为评估模式\n",
    "input_text = \"LLM with me\"  # 输入文本\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")  # 将文本编码为token索引\n",
    "temperature = 0.7  # 设置温度参数 & 一般设置为0到1之间的值\n",
    "generated_text = input_text + \" A:\"\n",
    "for _ in range(50):  # 假设我们想生成50个单词\n",
    "    with torch.no_grad():  # 不需要计算梯度\n",
    "        logits = model(input_ids)\n",
    "        logits = logits / temperature  # 应用温度调整\n",
    "        # 使用softmax函数将logits转换为概率分布 & 根据概率分布随机选择下一个单词\n",
    "        probabilities = F.softmax(logits[:, -1, :], dim=-1)\n",
    "        predicted_id = torch.multinomial(probabilities, num_samples=1)\n",
    "        # 将预测的token添加到输入序列中 & 将预测的token解码为文本并添加到生成的文本中\n",
    "        input_ids = torch.cat((input_ids, predicted_id), dim=1)\n",
    "        generated_text += tokenizer.decode(predicted_id[0])\n",
    "\n",
    "print(generated_text)\n",
    "eos_token = '<EOS>'  # 在生成文本后根据<EOS>进行切割\n",
    "generated_text_parts = generated_text.split(eos_token)\n",
    "final_text = generated_text_parts[0] + eos_token if len(generated_text_parts) > 1 else generated_text_parts[0]\n",
    "print(final_text)"
   ],
   "id": "805042cd80d3f658",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM with me A:waiLwantitLLMlovewantlearninerLarkgkinhitMterIwithlearningwantloMLninwldvklirnarLwaMveklokitlodi\n",
      "LLM with me A:waiLwantitLLMlovewantlearninerLarkgkinhitMterIwithlearningwantloMLninwldvklirnarLwaMveklokitlodi\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e6b58d356b545827"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
